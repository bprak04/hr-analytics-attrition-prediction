# -*- coding: utf-8 -*-
"""employee.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m_d7gTpNEW_aWUdYFhwdXTaEWvD3JT8z
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")
df.head()

# Attrition Count
sns.countplot(data=df, x='Attrition')
plt.title('Employee Attrition Count')
plt.show()

# Attrition by Department
sns.countplot(data=df, x='Department', hue='Attrition')
plt.title('Attrition by Department')
plt.xticks(rotation=45)
plt.show()

# Attrition by Gender
sns.countplot(data=df, x='Gender', hue='Attrition')
plt.title('Attrition by Gender')
plt.show()

# Heatmap of Correlation
df['Attrition_Binary'] = df['Attrition'].map({'Yes': 1, 'No': 0})
target_column = 'Attrition_Binary'

numeric_df = df.select_dtypes(include='number')
corr = numeric_df.corr()
target_corr = corr[target_column].drop(target_column).sort_values(ascending=False)
top_features = target_corr.abs().sort_values(ascending=False).head(10).index.tolist()

plt.figure(figsize=(10, 6))
sns.heatmap(numeric_df[top_features + [target_column]].corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap (Top Attrition Predictors)")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

X = df.select_dtypes(include='number').drop('Attrition_Binary', axis=1)
y = df['Attrition_Binary']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Logistic Regression
lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Logistic Regression")
print(classification_report(y_test, y_pred_lr))
print(confusion_matrix(y_test, y_pred_lr))
print("Accuracy:", accuracy_score(y_test, y_pred_lr))

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("\nRandom Forest")
print(classification_report(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))
print("Accuracy:", accuracy_score(y_test, y_pred_rf))

# XGBoost
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
print("\nXGBoost")
print(classification_report(y_test, y_pred_xgb))
print(confusion_matrix(y_test, y_pred_xgb))
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))

